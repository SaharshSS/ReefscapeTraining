Since I'm new to ChiefDelphi most the gifs will be on separate posts below this probably. :[
 
[NVIDIA Omniverse](https://www.nvidia.com/en-us/omniverse/) is a tool used for robotics simulation and [synthetic data generation](https://www.nvidia.com/en-us/use-cases/synthetic-data/). I believe it does have some practical use cases in FRC for quickly getting AI Models from data that would usually exist on full FRC Field that some people might not have access to. In the future I'll likely experiment to see how good it is for simulating robots *for my dream goal of a fully autonomous robot*.

You can generate synthetic data in various formats to do things such as
- Object pose estimation with [Isaac ROS Pose Estimation](https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_pose_estimation/index.html)
- Object detection through various methods such as YOLO and [Isaac ROS Object Detection](https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_object_detection/index.html)
- vSLAM
- So much more, see [here](https://nvidia-isaac-ros.github.io/repositories_and_packages/index.html)

*Note: the rest of this is kinda a very very bare-bones tutorial if you're looking at seeing what you can do and getting started*

For some prerequisites you should look [here](https://docs.omniverse.nvidia.com/isaacsim/latest/installation/requirements.html)

To get started you have to install the Omniverse Launcher, then you'll have to install Isaac Sim, and I think that version 4.1.0 is a little bit better just because of a bug in 4.2.0 that might make it hard to make custom materials. Even though you'll likely need some programming experience in python, if you have experience in any other language it'll likely not be too difficult for the light amount of code in python that you might write.

When you first open Isaac Sim you'll see something like:
![Isaac_Sim|690x389](upload://8CRrQQq6hULrb2Sxwa6QTcljnlT.jpeg)

Here you can drag a model of various potential fields to test the game on for me I wanted to play around with generating synthetic data for a real field for 2025, so after importing the Reefscape Field model I had to play around with the materials *(and still do a bit to get the most authentic feel)* and I got something that looked like this:
![Silly_Field|690x373](upload://nc7EUQDQNgEEsTugnqBixJ77SCL.jpeg)

Then I selected all the coral and all the algae and gave them semantics so when generating synthetic data it'll know what is what.

Then in a folder in vscode I created a little workspace that I called reefscape_sim which has a ```.vscode/``` folder that is fairly close to the one in ```C:\Users\*user*\AppData\Local\ov\pkg\isaac-sim-4.1.0\.vscode``` with some slight modifications so that I can have intellisense while I'm working.

Then my python code to generate some synthetic data follows as such:
```python
import numpy as np
import omni.isaac
import omni.kit.commands
import omni.usd
import omni.replicator.core as rep
from omni.isaac.core.utils.semantics import add_update_semantics
from pxr import Gf, Sdf, UsdGeom, Usd

stage = omni.usd.get_context().get_stage()
replicator_prim_path = "/Replicator"

# Check if the Replicator prim exists
replicator_prim = stage.GetPrimAtPath(replicator_prim_path)
if replicator_prim.IsValid():
    # Remove the Replicator prim and its children
    stage.RemovePrim(replicator_prim_path)
    print(f"Cleared existing Replicator prim at: {replicator_prim_path}")
else:
    print(f"No existing Replicator prim found at: {replicator_prim_path}")

base_field_prim_path = "/World/FE_2025/tn__FE2025_c9/tn__FullFidelityField_rHD"
coral_prim_paths = [(base_field_prim_path + "/Coral/Mesh_" + str(i)) for i in range(0, 150)]
coral_dynamic_prim_paths = [(base_field_prim_path + "/Coral/Mesh_" + str(i)) for i in range(126, 150)]
coral_static_prim_paths = [(base_field_prim_path + "/Coral/Mesh_" + str(i)) for i in range(0, 6)]
algae_static_prim_paths = [(base_field_prim_path + "/Algae/Mesh_" + str(i)) for i in range(0, 6)]
algae_prim_paths = [(base_field_prim_path + "/Algae/Mesh_" + str(i)) for i in range(0, 34)]

potential_look_prims = [
    base_field_prim_path + "/Barge/RedCageInner",
    base_field_prim_path + "/Barge/RedCageMiddle",
    base_field_prim_path + "/Barge/RedCageOuter",
    base_field_prim_path + "/Barge/BlueCageInner",
    base_field_prim_path + "/Barge/BlueCageMiddle",
    base_field_prim_path + "/Barge/BlueCageOuter"
] + coral_static_prim_paths + coral_dynamic_prim_paths + algae_prim_paths

camera = rep.create.camera()
with rep.trigger.on_frame(rt_subframes=4):
    with camera:
        rep.modify.pose(
            position=rep.distribution.uniform((-8.5, -4.2, 0.3), (8.5, 4.2, 5)),
            look_at=rep.distribution.choice(potential_look_prims)
        )

    light = rep.get.prim_at_path("/Environment/RectLight")
    with light:
        # scale_rand = rep.distribution.uniform(0.5, 1.5)
        # rep.modify.attribute(name="xformOp:scale", value=(scale_rand, scale_rand, scale_rand))
        rep.modify.attribute(name="inputs:colorTemperature", value=rep.distribution.normal(4500.0, 1500.0))
        rep.modify.attribute(name="inputs:intensity", value=rep.distribution.normal(0.0, 3500.0))
        rep.modify.attribute(name="inputs:color", value=rep.distribution.uniform((0.7, 0.7, 0.7), (1, 1, 1)))

    for i in range(0, 6):
        coral = rep.get.prim_at_path(coral_static_prim_paths[i])
        algae = rep.get.prim_at_path(algae_static_prim_paths[i])
        visible = rep.distribution.choice(choices=[True, False, False, False, False, False])
        rep.modify.visibility(visible, input_prims=coral)
        rep.modify.visibility(visible, input_prims=algae)
    for coral_prim_path in coral_prim_paths:
        if coral_prim_path not in coral_static_prim_paths:
            coral = rep.get.prim_at_path(coral_prim_path)
            with coral:
                rep.modify.visibility(rep.distribution.choice(choices=[True, False, False, False, False, False, False, False, False, False]))
    for algae_prim_path in algae_prim_paths:
        if algae_prim_path not in algae_static_prim_paths:
            algae = rep.get.prim_at_path(algae_prim_path)
            with algae:
                rep.modify.visibility(rep.distribution.choice(choices=[True, False, False, False, False]))
    

# Will render 512x512 images
render_product = rep.create.render_product(camera, (512, 512))

# Get a Kitti Writer and initialize its defaults
writer = rep.WriterRegistry.get("KittiWriter")
writer.initialize(
    output_dir=f"C:\\dev\\Omniverse\\Reefscape\\outv3\\",
    bbox_height_threshold=5,
    fully_visible_threshold=0.75,
    omit_semantic_type=True
)
# Attach render_product to the writer
writer.attach([render_product])

# Run the simulation graph
rep.orchestrator.run(num_frames=5120)
```
Heres a 1 minute long gif of 30 minutes of synthetic data generation happening:

![reefsim](upload://rXAFIwL33M14G2N6XxanDXy2uin.gif)
*Note: gif is heavily compressed and slowed down*

Afterwords for training a YOLO based model I take the data from my output folder and put it into a training workspace that I then run a script to convert all my Kitti data to YOLO. Finally i run a train.py script as such:
```python
import ultralytics
from ultralytics import YOLO

if __name__ == '__main__':
    print(f"Using Ultralytics v{ultralytics.__version__}")

    # Nano (n), Small (s), Medium (m), Large (l), Extra Large (x)
    model = YOLO(model="yolov8s.yaml") 

    model.train(data="conf.yaml", epochs=300, device="cuda", imgsz=640, patience=30)
```
with the conf.yaml of:
```yaml
path: C:/dev/Omniverse/Reefscape/training_workspace
train: images/train
val: images/val

names:
  0: algae
  1: coral
```

*Note: in the process before running the train.py it is likely fairly important to add some various real data to mix in with the synthetic ones (although I personally I have not yet). For me I would likely look at [Collections: First Robotics Competition dataset; the robot one specifically to mix in.](https://synodic.ai/frc)*
This is because as you can see in the gif it sometimes detects various other objects with only a single location for the images.

![model_test_iphone|230x500, 100%](upload://uZEY7JHuzlOJzwH0CnzKjZElRXN.gif)
*Note: This gif is significantly slowed down*
*Note_2: This was tested with [Vision Detector](https://apps.apple.com/us/app/vision-detector/id6443729650) on iOS by converting the YOLOv8s model to CoreML*

Since I'm on a Windows machine I had to use [WSL](https://learn.microsoft.com/en-us/windows/wsl/about) to convert the YOLO model to CoreML. The process followed as such: 
- Download python and install the ultralytics library if not installed yet
- Move the best.pt model to a folder in WSL.
- Create a python file, *in which I named conv.py*
- In the python file write the following and run:
```python
from ultralytics import YOLO

model = YOLO('best.pt')
model.export(format='coreml', nms=True)
```
- Move to best.mlpackage to iPhone through various means for use with *Vision Detector*

I'm still kinda new to using Omniverse and there's more to this but I thought to bring this to light to the best of my ability cause I found this really cool.

I think it'll be a good offseason project or potentially even during the season if you have spare time.
Although I love Omniverse and Isaac Sim I would say that the documentation is kind of sparse. There may be some issues that you'll come into that I didn't mention in this post but I'll be glad to try to help anyone if you get stuck. 
:3
